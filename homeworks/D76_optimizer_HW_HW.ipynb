{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "D76-optimizer_HW_HW.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2E-OVaISuOR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "be39e667-3bf9-461c-9241-1b9d0a1e2e48"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import os\n",
        "from keras import optimizers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmqQVBweS3iD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 20\n",
        "data_augmentation = True\n",
        "num_predictions = 20\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'keras_cifar10_trained_model.h5'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhX8SUnLS59U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "3432a4be-4784-47ce-97ba-d4443ea89502"
      },
      "source": [
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar2cQxWYS-Ka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "#   第二步：構建網絡層\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense( 10)) # 輸出結果是10個類別，所以維度是10   \n",
        "model.add(Activation('softmax')) # 最後一層用softmax作為激活函數"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwwIHirqS_op",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "1ee550bf-05c8-4ce4-a43f-4a6d09b50e1f"
      },
      "source": [
        "# 模型建立完成後，統計參數總量\n",
        "print(\"Total Parameters：%d\" % model.count_params())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Parameters：1250858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3fvOgPrTG86",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "outputId": "98f3426c-366c-44b9-cb94-ebc2aeea119f"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,250,858\n",
            "Trainable params: 1,250,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GITbwQkvTKlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#第三步編譯\n",
        "opt = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "#opt = optimizers.RMSprop()\n",
        "#opt = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
        "\n",
        "model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ICofFOVTMkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 資料正規化\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgUtgeSZTOx5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "outputId": "67a5dfbc-7b4e-446e-ea61-49c7497a7b3d"
      },
      "source": [
        "# 是否要做資料處理\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    history=model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    print('')\n",
        "        \n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "    history=model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)   \n",
        "\n",
        "'''\n",
        "   第四步：訓練\n",
        "   .fit的一些參數\n",
        "   batch_size：對總的樣本數進行分組，每組包含的樣本數量\n",
        "   epochs ：訓練次數\n",
        "   shuffle：是否把數據隨機打亂之後再進行訓練\n",
        "   validation_split：拿出百分之多少用來做交叉驗證\n",
        "   verbose：屏顯模式 - 0：不輸出, 1：輸出進度, 2：輸出每次的訓練結果\n",
        "''' \n",
        "    "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 24s 475us/step - loss: 1.7320 - accuracy: 0.3624 - val_loss: 1.3140 - val_accuracy: 0.5172\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 18s 350us/step - loss: 1.2882 - accuracy: 0.5365 - val_loss: 1.1061 - val_accuracy: 0.6003\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 17s 348us/step - loss: 1.1231 - accuracy: 0.6015 - val_loss: 0.9723 - val_accuracy: 0.6603\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 17s 348us/step - loss: 1.0132 - accuracy: 0.6419 - val_loss: 0.8881 - val_accuracy: 0.6894\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 17s 349us/step - loss: 0.9277 - accuracy: 0.6745 - val_loss: 0.8333 - val_accuracy: 0.7107\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 18s 351us/step - loss: 0.8696 - accuracy: 0.6957 - val_loss: 0.8301 - val_accuracy: 0.7108\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 17s 348us/step - loss: 0.8289 - accuracy: 0.7107 - val_loss: 0.7851 - val_accuracy: 0.7244\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 18s 350us/step - loss: 0.7895 - accuracy: 0.7222 - val_loss: 0.7364 - val_accuracy: 0.7469\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 18s 351us/step - loss: 0.7692 - accuracy: 0.7305 - val_loss: 0.7301 - val_accuracy: 0.7501\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 18s 355us/step - loss: 0.7461 - accuracy: 0.7397 - val_loss: 0.7106 - val_accuracy: 0.7590\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 18s 361us/step - loss: 0.7241 - accuracy: 0.7474 - val_loss: 0.7150 - val_accuracy: 0.7611\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 18s 355us/step - loss: 0.7019 - accuracy: 0.7544 - val_loss: 0.7347 - val_accuracy: 0.7515\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 17s 349us/step - loss: 0.6887 - accuracy: 0.7606 - val_loss: 0.7118 - val_accuracy: 0.7600\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 17s 347us/step - loss: 0.6790 - accuracy: 0.7633 - val_loss: 0.6969 - val_accuracy: 0.7621\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 17s 349us/step - loss: 0.6680 - accuracy: 0.7670 - val_loss: 0.6822 - val_accuracy: 0.7658\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 17s 349us/step - loss: 0.6629 - accuracy: 0.7684 - val_loss: 0.7098 - val_accuracy: 0.7631\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 18s 352us/step - loss: 0.6581 - accuracy: 0.7723 - val_loss: 0.7047 - val_accuracy: 0.7625\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 18s 350us/step - loss: 0.6445 - accuracy: 0.7756 - val_loss: 0.6839 - val_accuracy: 0.7695\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 18s 350us/step - loss: 0.6332 - accuracy: 0.7804 - val_loss: 0.7030 - val_accuracy: 0.7596\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 18s 350us/step - loss: 0.6308 - accuracy: 0.7825 - val_loss: 0.6703 - val_accuracy: 0.7761\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n   第四步：訓練\\n   .fit的一些參數\\n   batch_size：對總的樣本數進行分組，每組包含的樣本數量\\n   epochs ：訓練次數\\n   shuffle：是否把數據隨機打亂之後再進行訓練\\n   validation_split：拿出百分之多少用來做交叉驗證\\n   verbose：屏顯模式 - 0：不輸出, 1：輸出進度, 2：輸出每次的訓練結果\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtT_uhDqTRJJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "outputId": "706cdd08-3b0e-4a5a-9e95-b55b304354df"
      },
      "source": [
        "# 是否要做資料處理\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    history=model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    print('')\n",
        "        \n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "    history=model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 19s 387us/step - loss: 1.7117 - accuracy: 0.4578 - val_loss: 1.4421 - val_accuracy: 0.5072\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 19s 384us/step - loss: 1.7156 - accuracy: 0.4480 - val_loss: 1.7678 - val_accuracy: 0.4125\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 19s 381us/step - loss: 1.8051 - accuracy: 0.4296 - val_loss: 1.6149 - val_accuracy: 0.4459\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 19s 383us/step - loss: 1.8520 - accuracy: 0.4035 - val_loss: 1.8427 - val_accuracy: 0.3354\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 19s 383us/step - loss: 1.8215 - accuracy: 0.3941 - val_loss: 1.5673 - val_accuracy: 0.4495\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 19s 382us/step - loss: 1.8620 - accuracy: 0.3762 - val_loss: 1.4641 - val_accuracy: 0.4867\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 19s 382us/step - loss: 1.9157 - accuracy: 0.3639 - val_loss: 1.6402 - val_accuracy: 0.4690\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 19s 382us/step - loss: 1.8944 - accuracy: 0.3480 - val_loss: 1.7788 - val_accuracy: 0.3592\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 19s 381us/step - loss: 2.2305 - accuracy: 0.3205 - val_loss: 1.7429 - val_accuracy: 0.3067\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 19s 383us/step - loss: 2.6292 - accuracy: 0.3062 - val_loss: 1.8177 - val_accuracy: 0.3467\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 19s 383us/step - loss: 2.0217 - accuracy: 0.3010 - val_loss: 1.8516 - val_accuracy: 0.3133\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 20s 391us/step - loss: 2.0628 - accuracy: 0.2750 - val_loss: 1.9192 - val_accuracy: 0.2666\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 19s 381us/step - loss: 2.0935 - accuracy: 0.2644 - val_loss: 1.8240 - val_accuracy: 0.3382\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 19s 383us/step - loss: 2.1422 - accuracy: 0.2513 - val_loss: 1.7614 - val_accuracy: 0.3700\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 19s 380us/step - loss: 13.0300 - accuracy: 0.2418 - val_loss: 1.8716 - val_accuracy: 0.2669\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 19s 382us/step - loss: 3.0646 - accuracy: 0.2205 - val_loss: 1.9966 - val_accuracy: 0.2114\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 19s 380us/step - loss: 3.8722 - accuracy: 0.2032 - val_loss: 2.1059 - val_accuracy: 0.1795\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 19s 382us/step - loss: 2.2067 - accuracy: 0.1927 - val_loss: 1.8765 - val_accuracy: 0.2616\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 19s 380us/step - loss: 2.8833 - accuracy: 0.1948 - val_loss: 1.8208 - val_accuracy: 0.2772\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 19s 380us/step - loss: 2.1554 - accuracy: 0.1893 - val_loss: 2.0061 - val_accuracy: 0.1957\n",
            "10000/10000 [==============================] - 1s 150us/step\n",
            "Test loss: 2.0060737953186036\n",
            "Test accuracy: 0.195700004696846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksSdtYkETTWy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "3941438c-8d63-4a27-c4e3-26b8eec80025"
      },
      "source": [
        "#    第六步：輸出\n",
        "import numpy \n",
        "\n",
        "print ( \" test set \" )\n",
        "scores = model.evaluate(x_test,y_test,batch_size=200,verbose= 0)\n",
        "print ( \"\" )\n",
        "#print ( \" The test loss is %f \" % scores)\n",
        "print ( \" The test loss is %f \", scores)\n",
        "\n",
        "\n",
        "result = model.predict(x_test,batch_size=200,verbose= 0)\n",
        "\n",
        "result_max = numpy.argmax(result, axis = 1 )\n",
        "test_max = numpy.argmax(y_test, axis = 1 )\n",
        "\n",
        "result_bool = numpy.equal(result_max, test_max)\n",
        "true_num = numpy.sum(result_bool)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " test set \n",
            "\n",
            " The test loss is %f  [0.6702862191200256, 0.7760999798774719]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvxNDHpwedQd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "outputId": "6fea8da6-6f0e-4aa2-ff4b-6799d9ecd40a"
      },
      "source": [
        "#opt = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "opt = optimizers.RMSprop()\n",
        "#opt = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
        "model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# 是否要做資料處理\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    history=model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    print('')\n",
        "        \n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "    history=model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 20s 392us/step - loss: 1.6064 - accuracy: 0.3844 - val_loss: 1.5290 - val_accuracy: 0.4095\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 20s 390us/step - loss: 1.6176 - accuracy: 0.3884 - val_loss: 1.5226 - val_accuracy: 0.4168\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 20s 395us/step - loss: 1.6178 - accuracy: 0.3927 - val_loss: 1.4762 - val_accuracy: 0.4256\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 19s 384us/step - loss: 1.6254 - accuracy: 0.3973 - val_loss: 1.4504 - val_accuracy: 0.4401\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 19s 386us/step - loss: 1.6855 - accuracy: 0.4000 - val_loss: 1.5618 - val_accuracy: 0.4356\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 19s 382us/step - loss: 1.6193 - accuracy: 0.4103 - val_loss: 1.6145 - val_accuracy: 0.4043\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 19s 384us/step - loss: 1.6147 - accuracy: 0.4147 - val_loss: 1.5196 - val_accuracy: 0.4187\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 19s 384us/step - loss: 1.6185 - accuracy: 0.4161 - val_loss: 1.4573 - val_accuracy: 0.4634\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 19s 383us/step - loss: 1.7026 - accuracy: 0.4203 - val_loss: 1.7596 - val_accuracy: 0.3982\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 19s 382us/step - loss: 1.6384 - accuracy: 0.4167 - val_loss: 1.4482 - val_accuracy: 0.4600\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 19s 382us/step - loss: 1.6267 - accuracy: 0.4208 - val_loss: 1.4681 - val_accuracy: 0.4605\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 19s 382us/step - loss: 1.6272 - accuracy: 0.4232 - val_loss: 1.5859 - val_accuracy: 0.3977\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 19s 384us/step - loss: 1.6157 - accuracy: 0.4280 - val_loss: 1.5553 - val_accuracy: 0.4281\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 19s 380us/step - loss: 1.6332 - accuracy: 0.4214 - val_loss: 1.4639 - val_accuracy: 0.4657\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 19s 383us/step - loss: 1.6473 - accuracy: 0.4195 - val_loss: 1.4705 - val_accuracy: 0.4679\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 19s 383us/step - loss: 1.6500 - accuracy: 0.4191 - val_loss: 1.6186 - val_accuracy: 0.4136\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 19s 381us/step - loss: 1.7208 - accuracy: 0.4150 - val_loss: 1.7031 - val_accuracy: 0.4040\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 19s 382us/step - loss: 1.6544 - accuracy: 0.4108 - val_loss: 1.5157 - val_accuracy: 0.4474\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 19s 390us/step - loss: 1.6944 - accuracy: 0.4125 - val_loss: 1.6382 - val_accuracy: 0.3857\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 19s 383us/step - loss: 1.6586 - accuracy: 0.4126 - val_loss: 1.7144 - val_accuracy: 0.3432\n",
            "10000/10000 [==============================] - 1s 147us/step\n",
            "Test loss: 1.7143858108520509\n",
            "Test accuracy: 0.3431999981403351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IILpG6S4eoON",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "outputId": "243745df-e22f-4bd7-be1f-1d3e551bf469"
      },
      "source": [
        "#opt = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "#opt = optimizers.RMSprop()\n",
        "opt = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
        "model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# 是否要做資料處理\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    history=model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    print('')\n",
        "        \n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "    history=model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "    \n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 21s 415us/step - loss: 2.1260 - accuracy: 0.1699 - val_loss: 1.9160 - val_accuracy: 0.2213\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 20s 402us/step - loss: 2.1061 - accuracy: 0.1773 - val_loss: 1.8714 - val_accuracy: 0.2325\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 20s 404us/step - loss: 2.0273 - accuracy: 0.1876 - val_loss: 1.9894 - val_accuracy: 0.2113\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 20s 408us/step - loss: 2.0572 - accuracy: 0.1896 - val_loss: 2.3498 - val_accuracy: 0.1000\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 20s 408us/step - loss: 2.3090 - accuracy: 0.1010 - val_loss: 2.1104 - val_accuracy: 0.2065\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 20s 404us/step - loss: 2.0647 - accuracy: 0.1773 - val_loss: 1.8403 - val_accuracy: 0.2683\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 20s 403us/step - loss: 2.0145 - accuracy: 0.1912 - val_loss: 1.8661 - val_accuracy: 0.2236\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 21s 414us/step - loss: 2.0235 - accuracy: 0.1978 - val_loss: 2.0078 - val_accuracy: 0.2051\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 20s 403us/step - loss: 1.9886 - accuracy: 0.2007 - val_loss: 1.8609 - val_accuracy: 0.2150\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 20s 405us/step - loss: 2.0709 - accuracy: 0.1907 - val_loss: 1.8933 - val_accuracy: 0.2226\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 20s 401us/step - loss: 1.9569 - accuracy: 0.2216 - val_loss: 1.8018 - val_accuracy: 0.2651\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 20s 403us/step - loss: 1.9199 - accuracy: 0.2339 - val_loss: 1.7653 - val_accuracy: 0.2790\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 20s 401us/step - loss: 1.9362 - accuracy: 0.2309 - val_loss: 1.7296 - val_accuracy: 0.3068\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 20s 403us/step - loss: 1.8790 - accuracy: 0.2547 - val_loss: 1.8093 - val_accuracy: 0.2975\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 20s 400us/step - loss: 1.8609 - accuracy: 0.2638 - val_loss: 1.7275 - val_accuracy: 0.3050\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 20s 403us/step - loss: 1.8241 - accuracy: 0.2764 - val_loss: 1.7491 - val_accuracy: 0.2995\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 20s 401us/step - loss: 1.8014 - accuracy: 0.2871 - val_loss: 1.6294 - val_accuracy: 0.3527\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 20s 402us/step - loss: 1.7504 - accuracy: 0.3088 - val_loss: 1.6618 - val_accuracy: 0.3617\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 20s 402us/step - loss: 1.6980 - accuracy: 0.3371 - val_loss: 1.5278 - val_accuracy: 0.4127\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 20s 408us/step - loss: 1.6181 - accuracy: 0.3643 - val_loss: 1.5081 - val_accuracy: 0.4162\n",
            "10000/10000 [==============================] - 1s 146us/step\n",
            "Test loss: 1.508126668548584\n",
            "Test accuracy: 0.41620001196861267\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}